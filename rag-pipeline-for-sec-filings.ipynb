{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-08-12T17:42:30.063053Z","iopub.execute_input":"2025-08-12T17:42:30.063936Z","iopub.status.idle":"2025-08-12T17:42:30.404965Z","shell.execute_reply.started":"2025-08-12T17:42:30.063895Z","shell.execute_reply":"2025-08-12T17:42:30.404029Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pip install sec-parser sec-downloader beautifulsoup4 sentence-transformers chromadb google-generativeai requests lxml html2text\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-12T17:42:30.406374Z","iopub.execute_input":"2025-08-12T17:42:30.406863Z","iopub.status.idle":"2025-08-12T17:44:25.039260Z","shell.execute_reply.started":"2025-08-12T17:42:30.406835Z","shell.execute_reply":"2025-08-12T17:44:25.038158Z"},"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"GOOGLE_API_KEY=\"AIzaSyDq-Ayr0e9XBMc-4AO6DlUFvKwdk4Y2RWc\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-12T17:44:25.040632Z","iopub.execute_input":"2025-08-12T17:44:25.040979Z","iopub.status.idle":"2025-08-12T17:44:25.046340Z","shell.execute_reply.started":"2025-08-12T17:44:25.040941Z","shell.execute_reply":"2025-08-12T17:44:25.045384Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==== CELL A: INDEX (multiple tickers & multiple forms in a date range) ====\n\nimport os\nimport re\nimport time\nfrom datetime import datetime\nfrom typing import List, Dict, Any, Optional\n\nimport requests\nfrom bs4 import BeautifulSoup\n\n# --- sec-parser ---\nimport warnings\nimport sec_parser as sp\nfrom sec_parser.semantic_tree import TreeBuilder\nfrom sec_parser.semantic_elements import TopSectionTitle\nfrom sec_parser.processing_steps import (\n    TopSectionManagerFor10Q, IndividualSemanticElementExtractor, TopSectionTitleCheck\n)\n\n# --- vector store & embeddings ---\nimport chromadb\nfrom sentence_transformers import SentenceTransformer\nfrom transformers import AutoTokenizer\n\n# =========================\n# CONFIG\n# =========================\nUSER_AGENT = \"Your Name your_email@example.com\"   # <-- use a real UA\nCHROMA_DIR = \"./chroma_sec_multi\"                 # persistent store path\nEMBED_MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\nMAX_TOKENS = 512\nREQ_SLEEP = 0.25   # be kind to SEC endpoints (<= 10 req/sec)\n\nTICKERS = [\"AAPL\", \"MSFT\"]                        # <-- multi-ticker support\nFORMS   = [\"10-Q\", \"10-K\", \"8-K\", \"3\", \"4\", \"5\"]  # <-- choose which forms\nSTART_DATE = \"2024-01-01\"\nEND_DATE   = \"2025-08-01\"\n\n\n# =========================\n# SEC helpers (no API key)\n# =========================\ndef _headers():\n    return {\n        \"User-Agent\": USER_AGENT,\n        \"Accept-Encoding\": \"gzip, deflate\",\n        \"Accept-Language\": \"en-US,en;q=0.9\",\n        \"Connection\": \"keep-alive\",\n    }\n\ndef get_cik_for_ticker(ticker: str) -> Dict[str, str]:\n    url = \"https://www.sec.gov/files/company_tickers.json\"\n    r = requests.get(url, headers=_headers(), timeout=60)\n    r.raise_for_status()\n    data = r.json()\n    t_up = ticker.upper()\n    for _, row in data.items():\n        if row.get(\"ticker\", \"\").upper() == t_up:\n            cik = str(int(row[\"cik_str\"]))\n            return {\"cik\": cik, \"cik_padded\": f\"{int(cik):010d}\"}\n    raise ValueError(f\"Ticker {ticker} not found in SEC company_tickers.json\")\n\ndef list_filings_in_range(ticker: str, forms: List[str], start_date: str, end_date: str) -> List[Dict[str, str]]:\n    info = get_cik_for_ticker(ticker)\n    cik, cik_padded = info[\"cik\"], info[\"cik_padded\"]\n\n    def _collect(sub_json: dict, acc: List[Dict[str, str]]):\n        recent = sub_json.get(\"filings\", {}).get(\"recent\", {})\n        forms_list = recent.get(\"form\", [])\n        dates_list = recent.get(\"filingDate\", [])\n        accnos     = recent.get(\"accessionNumber\", [])\n        primary    = recent.get(\"primaryDocument\", [])\n        for f, d, a, p in zip(forms_list, dates_list, accnos, primary):\n            if not f or not d or not a or not p:\n                continue\n            # match any of our requested forms (prefix match for variants like \"10-K/A\", \"8-K/A\")\n            if not any(f.startswith(frm) for frm in forms):\n                continue\n            accnod = a.replace(\"-\", \"\")\n            url = f\"https://www.sec.gov/Archives/edgar/data/{cik}/{accnod}/{p}\"\n            acc.append({\"url\": url, \"filed_at\": d, \"accession\": a, \"form\": f, \"primary\": p})\n\n    # main submissions\n    base = f\"https://data.sec.gov/submissions/CIK{cik_padded}.json\"\n    r = requests.get(base, headers=_headers(), timeout=60)\n    r.raise_for_status()\n    sub = r.json()\n    time.sleep(REQ_SLEEP)\n\n    rows: List[Dict[str, str]] = []\n    _collect(sub, rows)\n\n    # older files index pages\n    for f in sub.get(\"filings\", {}).get(\"files\", []):\n        name = f.get(\"name\")\n        if not name:\n            continue\n        url = f\"https://data.sec.gov/submissions/{name}\"\n        rr = requests.get(url, headers=_headers(), timeout=60)\n        if rr.status_code == 200:\n            _collect(rr.json(), rows)\n        time.sleep(REQ_SLEEP)\n\n    # date filter inclusive\n    sd = datetime.strptime(start_date, \"%Y-%m-%d\").date()\n    ed = datetime.strptime(end_date, \"%Y-%m-%d\").date()\n    rows = [\n        r for r in rows\n        if sd <= datetime.strptime(r[\"filed_at\"], \"%Y-%m-%d\").date() <= ed\n    ]\n\n    # de-dup by accession; sort oldestâ†’newest\n    seen = set()\n    uniq = []\n    for r in sorted(rows, key=lambda x: x[\"filed_at\"]):\n        if r[\"accession\"] in seen:\n            continue\n        seen.add(r[\"accession\"])\n        uniq.append(r)\n    return uniq\n\ndef normalize_edgar_url(url: str) -> str:\n    # handle inline XBRL viewer links: /ix?doc=/Archives/...\n    from urllib.parse import urlparse, parse_qs\n    try:\n        p = urlparse(url)\n        if p.path.lower() == \"/ix\":\n            doc = parse_qs(p.query).get(\"doc\", [None])[0]\n            if doc:\n                return \"https://www.sec.gov\" + doc if doc.startswith(\"/\") else doc\n    except Exception:\n        pass\n    return url\n\ndef fetch_html(url: str) -> str:\n    url = normalize_edgar_url(url)\n    r = requests.get(url, headers=_headers(), timeout=90, allow_redirects=True)\n    r.raise_for_status()\n    time.sleep(REQ_SLEEP)\n    return r.text\n\n\n# =========================\n# Parsing by form\n# =========================\nITEM_RE = re.compile(r\"\\bItem\\s+\\d+[A]?(?:\\.\\d+)?\\b\", re.IGNORECASE)  # supports 8-K item 2.02 etc.\n\ndef _parse_items_10q(html: str) -> List[Dict[str, Any]]:\n    \"\"\"Full Item bodies for 10-Q using standard parser.\"\"\"\n    parser = sp.Edgar10QParser()\n    elements = parser.parse(html)\n    tree = TreeBuilder().build(elements)\n    nodes = list(tree.nodes)\n\n    # find Item titles\n    item_idx = []\n    for i, node in enumerate(nodes):\n        if isinstance(node.semantic_element, TopSectionTitle):\n            title = (node.text or \"\").strip()\n            if ITEM_RE.search(title):\n                item_idx.append((i, title))\n    if not item_idx:\n        for i, node in enumerate(nodes):\n            title = (node.text or \"\").strip()\n            if ITEM_RE.match(title):\n                item_idx.append((i, title))\n\n    sections = []\n    for j, (start_i, title) in enumerate(item_idx):\n        end_i = item_idx[j + 1][0] if j + 1 < len(item_idx) else len(nodes)\n        html_parts, text_parts = [], []\n        for k in range(start_i, end_i):\n            n = nodes[k]\n            try:\n                html_parts.append(n.get_source_code(pretty=False))\n            except Exception:\n                pass\n            t = (n.text or \"\").strip()\n            if t:\n                text_parts.append(t)\n        html_chunk = \"\".join(html_parts).strip()\n        text_chunk = (BeautifulSoup(html_chunk, \"html.parser\").get_text(separator=\"\\n\").strip()\n                      if html_chunk else \"\\n\".join(text_parts).strip())\n        m = re.search(r\"\\bItem\\s+(\\d+[A]?(?:\\.\\d+)?)\\b\", title, flags=re.IGNORECASE)\n        section_id = m.group(1).upper() if m else None\n        title_norm = re.sub(r\"\\s+\", \" \", title.replace(\"\\xa0\", \" \")).strip()\n        text_chunk = re.sub(r\"\\n{3,}\", \"\\n\\n\", text_chunk.replace(\"\\xa0\", \" \")).strip()\n        if text_chunk:\n            sections.append({\n                \"section_id\": section_id,\n                \"section_title\": title_norm,\n                \"html\": html_chunk or title_norm,\n                \"text\": text_chunk\n            })\n    return sections\n\ndef _get_steps_10k_generic():\n    \"\"\"\n    Remove 10-Q specific top-section logic as per sec-parser docs.\n    \"\"\"\n    all_steps = sp.Edgar10QParser().get_default_steps()\n    steps_wo_top_mgr = [st for st in all_steps if not isinstance(st, TopSectionManagerFor10Q)]\n    def get_checks_without_top_section_title_check():\n        checks = sp.Edgar10QParser().get_default_single_element_checks()\n        return [ck for ck in checks if not isinstance(ck, TopSectionTitleCheck)]\n    return [\n        IndividualSemanticElementExtractor(get_checks=get_checks_without_top_section_title_check)\n        if isinstance(st, IndividualSemanticElementExtractor) else st\n        for st in steps_wo_top_mgr\n    ]\n\ndef _parse_items_10k(html: str) -> List[Dict[str, Any]]:\n    \"\"\"Parse 10-K by skipping 10-Q-specific steps, then split by Item titles.\"\"\"\n    parser = sp.Edgar10QParser(get_steps=_get_steps_10k_generic)\n    with warnings.catch_warnings():\n        warnings.filterwarnings(\"ignore\", message=\"Invalid section type for\")\n        elements = parser.parse(html)\n    tree = TreeBuilder().build(elements)\n    nodes = list(tree.nodes)\n\n    # find Item titles (10-K has Item 1..15)\n    item_idx = []\n    for i, node in enumerate(nodes):\n        title = (node.text or \"\").strip()\n        if ITEM_RE.match(title):\n            item_idx.append((i, title))\n\n    sections = []\n    for j, (start_i, title) in enumerate(item_idx):\n        end_i = item_idx[j + 1][0] if j + 1 < len(item_idx) else len(nodes)\n        html_parts, text_parts = [], []\n        for k in range(start_i, end_i):\n            n = nodes[k]\n            try:\n                html_parts.append(n.get_source_code(pretty=False))\n            except Exception:\n                pass\n            t = (n.text or \"\").strip()\n            if t:\n                text_parts.append(t)\n        html_chunk = \"\".join(html_parts).strip()\n        text_chunk = (BeautifulSoup(html_chunk, \"html.parser\").get_text(separator=\"\\n\").strip()\n                      if html_chunk else \"\\n\".join(text_parts).strip())\n        m = re.search(r\"\\bItem\\s+(\\d+[A]?)\\b\", title, flags=re.IGNORECASE)\n        section_id = m.group(1).upper() if m else None\n        title_norm = re.sub(r\"\\s+\", \" \", title.replace(\"\\xa0\", \" \")).strip()\n        text_chunk = re.sub(r\"\\n{3,}\", \"\\n\\n\", text_chunk.replace(\"\\xa0\", \" \")).strip()\n        if text_chunk:\n            sections.append({\n                \"section_id\": section_id,\n                \"section_title\": title_norm,\n                \"html\": html_chunk or title_norm,\n                \"text\": text_chunk\n            })\n    return sections\n\ndef _parse_items_8k(html: str) -> List[Dict[str, Any]]:\n    \"\"\"\n    8-K Items are like 'Item 2.02 Results of Operations and Financial Condition'.\n    We'll split on these titles similarly.\n    \"\"\"\n    # Reuse 10-K generic parser (no 10-Q top manager)\n    parser = sp.Edgar10QParser(get_steps=_get_steps_10k_generic)\n    with warnings.catch_warnings():\n        warnings.filterwarnings(\"ignore\", message=\"Invalid section type for\")\n        elements = parser.parse(html)\n    tree = TreeBuilder().build(elements)\n    nodes = list(tree.nodes)\n\n    item_idx = []\n    for i, node in enumerate(nodes):\n        title = (node.text or \"\").strip()\n        if re.match(r\"\\bItem\\s+\\d+\\.\\d+\\b\", title, flags=re.IGNORECASE):\n            item_idx.append((i, title))\n\n    # fallback: any \"Item x.xx\" found anywhere\n    if not item_idx:\n        for i, node in enumerate(nodes):\n            title = (node.text or \"\").strip()\n            if re.search(r\"\\bItem\\s+\\d+\\.\\d+\\b\", title, flags=re.IGNORECASE):\n                item_idx.append((i, title))\n\n    sections = []\n    for j, (start_i, title) in enumerate(item_idx):\n        end_i = item_idx[j + 1][0] if j + 1 < len(item_idx) else len(nodes)\n        html_parts, text_parts = [], []\n        for k in range(start_i, end_i):\n            n = nodes[k]\n            try:\n                html_parts.append(n.get_source_code(pretty=False))\n            except Exception:\n                pass\n            t = (n.text or \"\").strip()\n            if t:\n                text_parts.append(t)\n        html_chunk = \"\".join(html_parts).strip()\n        text_chunk = (BeautifulSoup(html_chunk, \"html.parser\").get_text(separator=\"\\n\").strip()\n                      if html_chunk else \"\\n\".join(text_parts).strip())\n        m = re.search(r\"\\bItem\\s+(\\d+\\.\\d+)\\b\", title, flags=re.IGNORECASE)\n        section_id = m.group(1) if m else None\n        title_norm = re.sub(r\"\\s+\", \" \", title.replace(\"\\xa0\", \" \")).strip()\n        text_chunk = re.sub(r\"\\n{3,}\", \"\\n\\n\", text_chunk.replace(\"\\xa0\", \" \")).strip()\n        if text_chunk:\n            sections.append({\n                \"section_id\": section_id,\n                \"section_title\": title_norm,\n                \"html\": html_chunk or title_norm,\n                \"text\": text_chunk\n            })\n    return sections\n\ndef _parse_whole_doc(html: str) -> List[Dict[str, Any]]:\n    \"\"\"For Forms 3/4/5 (insider forms), treat the entire doc as a single section.\"\"\"\n    soup = BeautifulSoup(html, \"html.parser\")\n    text = soup.get_text(separator=\"\\n\").strip()\n    text = re.sub(r\"\\n{3,}\", \"\\n\\n\", text.replace(\"\\xa0\", \" \"))\n    if not text:\n        return []\n    return [{\n        \"section_id\": \"ALL\",\n        \"section_title\": \"Entire Document\",\n        \"html\": html,\n        \"text\": text\n    }]\n\ndef parse_sections_by_form(html: str, form: str) -> List[Dict[str, Any]]:\n    base = form.upper()\n    if base.startswith(\"10-Q\"):\n        return _parse_items_10q(html)\n    if base.startswith(\"10-K\"):\n        return _parse_items_10k(html)\n    if base.startswith(\"8-K\"):\n        return _parse_items_8k(html)\n    if base in {\"3\", \"4\", \"5\"}:\n        return _parse_whole_doc(html)\n    # default fallback\n    return _parse_whole_doc(html)\n\n\n# =========================\n# Chunk, embed, upsert (metadata inside embedding text)\n# =========================\ndef chunk_text(s: str, max_chars: int = 1800, overlap: int = 200) -> List[str]:\n    s = s.strip()\n    if len(s) <= max_chars:\n        return [s]\n    chunks, start = [], 0\n    while start < len(s):\n        end = min(len(s), start + max_chars)\n        split = s.rfind(\"\\n\\n\", start, end)\n        if split == -1 or split <= start + 200:\n            split = end\n        chunks.append(s[start:split].strip())\n        if split == len(s):\n            break\n        start = max(split - overlap, split)\n    return [c for c in chunks if c]\n\ndef _trim_to_tokens(text: str, tokenizer, max_tokens: int) -> str:\n    enc = tokenizer(text, add_special_tokens=False, return_attention_mask=False, return_token_type_ids=False)\n    ids = enc[\"input_ids\"]\n    if len(ids) <= max_tokens:\n        return text\n    trimmed = tokenizer.decode(ids[:max_tokens], skip_special_tokens=True, clean_up_tokenization_spaces=True)\n    return trimmed.strip()\n\ndef build_chroma(persist_dir: str = CHROMA_DIR):\n    client = chromadb.PersistentClient(path=persist_dir)\n    coll = client.get_or_create_collection(name=\"sec_multi_forms\")\n    return client, coll\n\ndef upsert_sections_to_chroma(\n    coll,\n    sections: List[Dict[str, Any]],\n    ticker: str,\n    filed_at: str,\n    form: str,\n    model: SentenceTransformer,\n    tokenizer: AutoTokenizer,\n    max_tokens: int = MAX_TOKENS,\n    store_combined_text: bool = False\n) -> int:\n    ids, docs, metas, embs = [], [], [], []\n    uid = 0\n\n    for s in sections:\n        base_meta = {\n            \"ticker\": ticker,\n            \"form\": form,\n            \"filed_at\": filed_at,\n            \"section_id\": s.get(\"section_id\"),\n            \"section_title\": s.get(\"section_title\"),\n        }\n        meta_str = (\n            f\"[TICKER={ticker}|FORM={form}|DATE={filed_at}\"\n            f\"{'|SEC='+str(s.get('section_id')) if s.get('section_id') else ''}] \"\n            f\"{(s.get('section_title') or '')[:120]}\"\n        ).strip()\n\n        for ci, chunk in enumerate(chunk_text(s[\"text\"])):\n            reserve = 2\n            meta_tokens_ct = len(tokenizer(meta_str, add_special_tokens=False)[\"input_ids\"])\n            remaining = max_tokens - min(meta_tokens_ct, max_tokens) - reserve\n            chunk_trimmed = _trim_to_tokens(chunk, tokenizer, max(remaining, 0))\n            combined = f\"{meta_str}\\n\\n{chunk_trimmed}\".strip()\n\n            emb = model.encode(combined, normalize_embeddings=True).tolist()\n            doc_text = combined if store_combined_text else chunk\n\n            meta = dict(base_meta)\n            meta[\"chunk_index\"] = ci\n\n            ids.append(f\"{ticker}_{form}_{filed_at}_{s.get('section_id')}_{uid:06d}\")\n            docs.append(doc_text)\n            metas.append(meta)\n            embs.append(emb)\n            uid += 1\n\n    if ids:\n        coll.upsert(ids=ids, documents=docs, metadatas=metas, embeddings=embs)\n    return len(ids)\n\n\n# =========================\n# RUN: iterate tickers & forms over date range\n# =========================\nclient, collection = build_chroma(CHROMA_DIR)\nmodel = SentenceTransformer(EMBED_MODEL_NAME)\ntokenizer = AutoTokenizer.from_pretrained(EMBED_MODEL_NAME)\n\ntotal = 0\nfor tkr in TICKERS:\n    filings = list_filings_in_range(tkr, FORMS, START_DATE, END_DATE)\n    print(f\"\\n{tkr}: {len(filings)} filings from {START_DATE} to {END_DATE}\")\n    for f in filings:\n        print(\"  \", f[\"filed_at\"], f[\"form\"], \"â†’\", f[\"url\"])\n        html = fetch_html(f[\"url\"])\n        sections = parse_sections_by_form(html, f[\"form\"])\n        inserted = upsert_sections_to_chroma(\n            collection, sections, tkr, filed_at=f[\"filed_at\"], form=f[\"form\"],\n            model=model, tokenizer=tokenizer\n        )\n        total += inserted\n        print(f\"    upserted {inserted} chunks\")\n\nprint(f\"\\nDONE. Total chunks upserted: {total}. DB: {CHROMA_DIR}\")\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-12T17:44:25.048756Z","iopub.execute_input":"2025-08-12T17:44:25.049004Z","iopub.status.idle":"2025-08-12T17:53:19.122468Z","shell.execute_reply.started":"2025-08-12T17:44:25.048984Z","shell.execute_reply":"2025-08-12T17:53:19.121410Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==== CELL B: QUERY (load from Chroma, retrieve top-k, call Gemini) ====\n\nimport os\nimport json\nfrom typing import List, Dict, Any, Optional\nfrom sentence_transformers import SentenceTransformer\nfrom transformers import AutoTokenizer\nimport chromadb\nimport google.generativeai as genai\n\n# Reuse the same settings used at index time\nCHROMA_DIR = \"./chroma_10q\"\nEMBED_MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\nGEMINI_MODEL = \"gemini-2.5-flash\"   # or \"gemini-1.5-pro\"\nTOP_K = 2\n\ndef load_chroma():\n    client = chromadb.PersistentClient(path=CHROMA_DIR)\n    return client.get_or_create_collection(name=\"sec_10q_sections\")\n\ndef retrieve_top_k(coll, query: str, k: int = TOP_K):\n    model = SentenceTransformer(EMBED_MODEL_NAME)\n    q_emb = model.encode(query, normalize_embeddings=True).tolist()\n    res = coll.query(query_embeddings=[q_emb], n_results=k)\n    hits = []\n    if res and res.get(\"documents\"):\n        for i in range(len(res[\"documents\"][0])):\n            hits.append({\n                \"id\": res[\"ids\"][0][i],\n                \"text\": res[\"documents\"][0][i],\n                \"meta\": res[\"metadatas\"][0][i],\n                \"distance\": res[\"distances\"][0][i],  # cosine distance (smaller is better)\n            })\n    hits.sort(key=lambda x: x[\"distance\"])\n    return hits\n\ndef gemini_answer(query: str, contexts: List[Dict[str, Any]]) -> str:\n    api_key = GOOGLE_API_KEY\n    if not api_key:\n        raise RuntimeError(\"Set GOOGLE_API_KEY env var for Gemini.\")\n    genai.configure(api_key=api_key)\n    model = genai.GenerativeModel(GEMINI_MODEL)\n\n    blocks = []\n    for c in contexts:\n        m = c.get(\"meta\", {})\n        tag = f\"[{m.get('ticker')} | {m.get('form')} | {m.get('filed_at')} | {m.get('section_title')} | chunk {m.get('chunk_index')}]\"\n        blocks.append(f\"{tag}\\n{c['text']}\\n\")\n\n    prompt = (\n        \"You are a financial research assistant.\\n\"\n        \"Use the provided SEC 10-Q excerpts to answer the user question. \"\n        \"Cite the section titles inline, and be concise. If uncertain, say so.\\n\\n\"\n        \"whenever numerical values need to be analysed or compared give output in markdown tabular format\"\n        f\"USER QUESTION:\\n{query}\\n\\n\" +\n        \"\\n\".join(f\"CONTEXT {i+1}:\\n{blk}\" for i, blk in enumerate(blocks[:TOP_K]))\n    )\n\n    resp = model.generate_content(prompt)\n    return resp.text\n\n# ======= RUN THIS CELL MULTIPLE TIMES =======\ncollection = load_chroma()\n\nuser_query = \"Based on the data you have is there any data for apple\"\nhits = retrieve_top_k(collection, user_query, k=5)\nprint(\"Top hits:\\n\", json.dumps(hits, indent=2))\n\nanswer = gemini_answer(user_query, hits[:2])\nprint(\"\\n=== GEMINI ANSWER ===\\n\")\nprint(answer)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-12T18:13:42.684783Z","iopub.execute_input":"2025-08-12T18:13:42.685137Z","iopub.status.idle":"2025-08-12T18:13:53.257117Z","shell.execute_reply.started":"2025-08-12T18:13:42.685103Z","shell.execute_reply":"2025-08-12T18:13:53.255763Z"}},"outputs":[],"execution_count":null}]}